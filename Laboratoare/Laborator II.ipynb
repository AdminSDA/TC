{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza sintactica\n",
    "\n",
    "In laboratorul trecut am vazut cum putem extrage tokenii dintr-un text. Folosind doar analiza lexicala nu putem verifica daca tokenii respecta o anumita ordine, ci doar daca exista. Pentru a verifica daca avem si o ordine corecta a tokenilor trebuie sa construim o gramatica care verifica daca un anumit text ar fi putut fi generat de ea.\n",
    "\n",
    "Exemplu: Se dau mai multe liste de numere de forma [1, 2, 3]; [4, 5, 6]; ... ; [7, 8, 9, 10]. Calculati suma lor.\n",
    "\n",
    "In primul rand trebuie sa gasim tokenii: ([, ], ',' , digit, spatiu*, ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sly import Lexer, Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsLexer(Lexer):\n",
    "    tokens = { OPEN_BRACE, CLOSE_BRACE, COMMA, DIGITS, SEMICOLON }\n",
    "    ignore = ' \\t'             \n",
    "\n",
    "    OPEN_BRACE = r'\\['\n",
    "    CLOSE_BRACE = r'\\]'\n",
    "    COMMA = ','\n",
    "    DIGITS = r'\\d+'\n",
    "    SEMICOLON = r';'\n",
    "    \n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='OPEN_BRACE', value='[', lineno=1, index=1)\n",
      "Token(type='DIGITS', value='1', lineno=1, index=2)\n",
      "Token(type='COMMA', value=',', lineno=1, index=3)\n",
      "Token(type='DIGITS', value='2', lineno=1, index=5)\n",
      "Token(type='COMMA', value=',', lineno=1, index=6)\n",
      "Token(type='DIGITS', value='3', lineno=1, index=8)\n",
      "Token(type='CLOSE_BRACE', value=']', lineno=1, index=9)\n",
      "Token(type='SEMICOLON', value=';', lineno=1, index=10)\n",
      "Token(type='OPEN_BRACE', value='[', lineno=1, index=12)\n",
      "Token(type='DIGITS', value='4', lineno=1, index=13)\n",
      "Token(type='COMMA', value=',', lineno=1, index=14)\n",
      "Token(type='DIGITS', value='5', lineno=1, index=16)\n",
      "Token(type='COMMA', value=',', lineno=1, index=17)\n",
      "Token(type='DIGITS', value='6', lineno=1, index=19)\n",
      "Token(type='CLOSE_BRACE', value=']', lineno=1, index=20)\n",
      "Token(type='SEMICOLON', value=';', lineno=1, index=21)\n",
      "Token(type='OPEN_BRACE', value='[', lineno=1, index=23)\n",
      "Token(type='DIGITS', value='7', lineno=1, index=24)\n",
      "Token(type='COMMA', value=',', lineno=1, index=25)\n",
      "Token(type='DIGITS', value='8', lineno=1, index=27)\n",
      "Token(type='COMMA', value=',', lineno=1, index=28)\n",
      "Token(type='DIGITS', value='9', lineno=1, index=30)\n",
      "Token(type='COMMA', value=',', lineno=1, index=31)\n",
      "Token(type='DIGITS', value='10', lineno=1, index=33)\n",
      "Token(type='CLOSE_BRACE', value=']', lineno=1, index=35)\n",
      "Token(type='SEMICOLON', value=';', lineno=1, index=36)\n"
     ]
    }
   ],
   "source": [
    "input_text = ' [1, 2, 3]; [4, 5, 6]; [7, 8, 9, 10];'\n",
    "lexer = ListsLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "Observam ca putem extrage tokenii, dar se poate sa nu fie in ordinea corecta (de ex: [[,123,]1[]). Implementam parser-ul.\n",
    "\n",
    "Pentru parser trebuie mai intai sa gasim o gramatica care recunoaste liste scrise corect:\n",
    "\n",
    "<ul>\n",
    "    <li>lists -> list | list lists</li>\n",
    "    <li>list -> OPEN_BRACE numbers CLOSE_BRACE SEMICOLON</li>\n",
    "    <li>numbers -> DIGITS | numbers COMMA DIGITS</li>\n",
    "</ul>\n",
    "\n",
    "Explicatie:\n",
    "<ul>\n",
    "    <li>Un input corect (mai multe liste) este compus fie dintr-o singura lista, fie este o lista urmata de alte liste</li>\n",
    "    <li>O lista incepe cu [, dupa care urmeaza mai multe numere, apoi trebuie sa apara ] si punct si virgula (;)</li>\n",
    "    <li>Numerele pot fi reprezentate de un singur numar (DIGITS) sau sa apara mai multe numere (numbers) urmate de virgula si de un numar</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Implementam parser-ul scriind pentru fiecare productie (A -> BC) cate o regula si ce trebuie sa faca parser-ul cand identifica regula.\n",
    "\n",
    "Pentru fiecare regula scriem cate o functie care este decorata \\(@_\\) cu  partea dreapta a productiei. Numele functiei este dat de partea stanga a productiei. De exemplu pentru productia \"numbers -> DIGITS\" avem urmatoarea definitie de functie:\n",
    "\n",
    "<block>\n",
    "@_('DIGITS')<br>\n",
    "def numbers(self, p):<br>\n",
    "</block>\n",
    "\n",
    "Parametrul p (productie) al functiei numbers ne permite sa accesam partea dreapta a productiei (in cazul de fata avem acces la p.DIGITS care reprezinta pe rand numere (1, 4 si 7; de ce nu si restul numerelor?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListsParser(Parser):\n",
    "    tokens = ListsLexer.tokens\n",
    "        \n",
    "    # lists -> list\n",
    "    @_('list')\n",
    "    def lists(self, p):\n",
    "        return [p.list]\n",
    "    \n",
    "    # lists -> list lists\n",
    "    @_('list lists')\n",
    "    def lists(self, p):\n",
    "        current_list = p.list\n",
    "        rest_of_lists = list(p.lists)\n",
    "        \n",
    "        all_lists = [current_list] + rest_of_lists\n",
    "            \n",
    "        return all_lists\n",
    "    \n",
    "    # list -> OPEN_BRACE numbers CLOSE_BRACE SEMICOLON\n",
    "    @_('OPEN_BRACE numbers CLOSE_BRACE SEMICOLON')\n",
    "    def list(self, p):\n",
    "        return p.numbers\n",
    "    \n",
    "    # numbers -> DIGITS\n",
    "    @_('DIGITS')\n",
    "    def numbers(self, p):\n",
    "        return [int(p.DIGITS)]\n",
    "\n",
    "    # numbers -> numbers COMMA DIGITS\n",
    "    @_('numbers COMMA DIGITS')\n",
    "    def numbers(self, p):\n",
    "        current_number = int(p.DIGITS)\n",
    "        current_list = list(p.numbers)\n",
    "        \n",
    "        current_list.append(current_number)\n",
    "        return current_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "input_text = ' [1, 2, 3]; [4, 5, 6]; [7, 8, 9, 10];'\n",
    "\n",
    "lexer = ListsLexer()\n",
    "parser = ListsParser()\n",
    "\n",
    "lists = parser.parse(lexer.tokenize(input_text))\n",
    "print(sum(map(sum, lists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu.\n",
    "\n",
    "Modificati exemplul anterior astfel incat sa returnati direct suma numerelor, fara a mai salva listele intermediare. Hint: puteti defini un constructor pentru clasa ListsParser si sa folositi variabile de instanta (de exemplu self.sum = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Modificati exemplul anterior astfel incat sa efectuati suma fiecarei liste si apoi produsul sumelor. Exemplu pentru: [1, 2, 3]; [4, 5, 6]; [7, 8, 9, 10]; sumele sunt 6, 15 si 34, iar produsul este 3060."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu.\n",
    "\n",
    "Primind un cod de tipul foreach(int x : v[1, n]) translatati-l in codul echivalentul pentru C (atentie la x, v, 1, n). Rezultatul ar trebui sa fie un string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='TOKENS', value='f', lineno=1, index=0)\n",
      "Token(type='TOKENS', value='o', lineno=1, index=1)\n",
      "Token(type='TOKENS', value='r', lineno=1, index=2)\n",
      "Token(type='TOKENS', value='e', lineno=1, index=3)\n",
      "Token(type='TOKENS', value='a', lineno=1, index=4)\n",
      "Token(type='TOKENS', value='c', lineno=1, index=5)\n",
      "Token(type='TOKENS', value='h', lineno=1, index=6)\n",
      "Token(type='TOKENS', value='(', lineno=1, index=7)\n",
      "Token(type='TOKENS', value='i', lineno=1, index=8)\n",
      "Token(type='TOKENS', value='n', lineno=1, index=9)\n",
      "Token(type='TOKENS', value='t', lineno=1, index=10)\n",
      "Token(type='TOKENS', value='x', lineno=1, index=12)\n",
      "Token(type='TOKENS', value=':', lineno=1, index=14)\n",
      "Token(type='TOKENS', value='v', lineno=1, index=16)\n",
      "Token(type='TOKENS', value='[', lineno=1, index=17)\n",
      "Token(type='TOKENS', value='1', lineno=1, index=18)\n",
      "Token(type='TOKENS', value=',', lineno=1, index=19)\n",
      "Token(type='TOKENS', value='n', lineno=1, index=21)\n",
      "Token(type='TOKENS', value=']', lineno=1, index=22)\n",
      "Token(type='TOKENS', value=')', lineno=1, index=23)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yacc: Syntax error at line 1, token=TOKENS\n"
     ]
    }
   ],
   "source": [
    "class SyntacticSugarLexer(Lexer):\n",
    "    tokens = { TOKENS }\n",
    "    ignore = ' \\t'             \n",
    "\n",
    "    # Modify TOKENS\n",
    "    TOKENS = r'.'\n",
    "    \n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1\n",
    "        \n",
    "class SyntacticSugarParser(Parser):\n",
    "    tokens = SyntacticSugarLexer.tokens\n",
    "        \n",
    "    @_('TOKENS')\n",
    "    def rule(self, p):\n",
    "        return p.TOKENS\n",
    "    \n",
    "input_text = 'foreach(int x : v[1, n])'\n",
    "\n",
    "''' Lexer test '''\n",
    "lexer = SyntacticSugarLexer()\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "\n",
    "''' Parser test '''\n",
    "lexer = SyntacticSugarLexer()\n",
    "parser = SyntacticSugarParser()\n",
    "\n",
    "print(parser.parse(lexer.tokenize(input_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Construiti un **parser** care verifica daca primeste cuvinte de tipul a^n b^n cu n >= 1. Exemplu: aabbb => NU, aaabbb => DA, ab => DA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: no grammar rules are defined\n"
     ]
    },
    {
     "ename": "YaccError",
     "evalue": "Invalid grammar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mYaccError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-9e0eedeee959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAnBnParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnBnLexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(meta, clsname, bases, attributes)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(cls, definitions)\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0;31m# Build the underlying grammar object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_grammar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mYaccError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid grammar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;31m# Build the LR tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mYaccError\u001b[0m: Invalid grammar"
     ]
    }
   ],
   "source": [
    "class AnBnLexer(Lexer):\n",
    "    tokens = { ''' TODO ''' }\n",
    "    ignore = ' \\t'             \n",
    "\n",
    "    ''' TODO '''\n",
    "    \n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1\n",
    "        \n",
    "class AnBnParser(Parser):\n",
    "    tokens = AnBnLexer.tokens\n",
    "        \n",
    "    ''' TODO '''\n",
    "    \n",
    "    \n",
    "input_text = 'aaabbbb'\n",
    "\n",
    "''' Lexer test '''\n",
    "lexer = AnBnLexer()\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "\n",
    "''' Parser test '''\n",
    "lexer = AnBnLexer()\n",
    "parser = AnBnParser()\n",
    "\n",
    "print(parser.parse(lexer.tokenize(input_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Construiti un parser care sa returneze o lista de valori de tip bool, care reprezinta daca un cuvant este palindrom sau nu. Pentru input-ul \"abba aca accas a\", parser-ul trebuie sa returneze [True, True, False, True]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: no grammar rules are defined\n"
     ]
    },
    {
     "ename": "YaccError",
     "evalue": "Invalid grammar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mYaccError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-eebf340d79dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPalindromicParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnBnLexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(meta, clsname, bases, attributes)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(cls, definitions)\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0;31m# Build the underlying grammar object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_grammar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mYaccError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid grammar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;31m# Build the LR tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mYaccError\u001b[0m: Invalid grammar"
     ]
    }
   ],
   "source": [
    "class PalindromicLexer(Lexer):\n",
    "    tokens = { ''' TODO ''' }\n",
    "    ignore = ' \\t'             \n",
    "\n",
    "    ''' TODO '''\n",
    "    \n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1\n",
    "        \n",
    "class PalindromicParser(Parser):\n",
    "    tokens = AnBnLexer.tokens\n",
    "        \n",
    "    ''' TODO '''\n",
    "    \n",
    "    \n",
    "input_text = 'abba aca accas a'\n",
    "\n",
    "''' Lexer test '''\n",
    "lexer = PalindromicLexer()\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "\n",
    "''' Parser test '''\n",
    "lexer = PalindromicLexer()\n",
    "parser = PalindromicParser()\n",
    "\n",
    "print(parser.parse(lexer.tokenize(input_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Construiti un parser care primeste o expresie in forma infix si returneaza expresia in forma postfixata. \n",
    "\n",
    "Exemplu pentru input-ul: (a+b)\\*c, parser-ul returneaza ab+c*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: no grammar rules are defined\n"
     ]
    },
    {
     "ename": "YaccError",
     "evalue": "Invalid grammar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mYaccError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-be1f93e0ec7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInfixToPostfixParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnBnLexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(meta, clsname, bases, attributes)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/yacc.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(cls, definitions)\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0;31m# Build the underlying grammar object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_grammar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mYaccError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid grammar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;31m# Build the LR tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mYaccError\u001b[0m: Invalid grammar"
     ]
    }
   ],
   "source": [
    "class InfixToPostfixLexer(Lexer):\n",
    "    tokens = { ''' TODO ''' }\n",
    "    ignore = ' \\t'             \n",
    "\n",
    "    ''' TODO '''\n",
    "    \n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1\n",
    "        \n",
    "class InfixToPostfixParser(Parser):\n",
    "    tokens = AnBnLexer.tokens\n",
    "        \n",
    "    ''' TODO '''\n",
    "    \n",
    "    \n",
    "input_text = 'abba aca accas a'\n",
    "\n",
    "''' Lexer test '''\n",
    "lexer = InfixToPostfixLexer()\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "\n",
    "''' Parser test '''\n",
    "lexer = InfixToPostfixLexer()\n",
    "parser = InfixToPostfixParser()\n",
    "\n",
    "print(parser.parse(lexer.tokenize(input_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
