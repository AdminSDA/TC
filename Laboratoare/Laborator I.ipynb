{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tehnici de compilare\n",
    "\n",
    "In cursul acestui laborator vom studia si implementa algoritmi care sunt folositi de compilatoare.\n",
    "\n",
    "Dupa cum stiti, cand folosim un limbaj **static** de nivel inalt (C, C++, Java etc.) voi scrieti **cod sursa** intr-un fisier (de ex: main.c, Vector.cpp, WebBrowser.java) pe care il pasati unui **compilator** care citeste fisierul si incearca sa vada daca are sens. \n",
    "\n",
    "Dupa aceea, daca nu apar erori de compilare (de ex: int a = \"ABC\";), compilatorul produce un **fisier executabil**. Voi rulati acel fisier si in cazul in care output-ul este gresit, modificati codul sursa, urmand sa compilati din nou fisierul si sa rulati noul executabil.\n",
    "\n",
    "La acest laborator vom studia doua parti importante ale compilatoarelor: **analiza lexicala** si **analiza sintactica**.\n",
    "\n",
    "### Analiza lexicala\n",
    "\n",
    "**DEX**: LÉXIC s. n. ***Totalitatea cuvintelor*** dintr-o limbă; vocabular.\n",
    "\n",
    "Pentru ca un compilator sa stie cum sa **parseze** un fisier text, el trebuie sa aiba o lista a cuvintelor cheie care sunt folosite de limbaj (ex: <a href=\"https://www.w3schools.in/c-tutorial/keywords/\">C Keywords</a>, <a href=\"https://www.w3schools.in/cplusplus-tutorial/keywords/\">C++ Keywords</a>, <a href=\"https://www.w3schools.com/python/python_ref_keywords.asp\">Python Keywords</a>).\n",
    "\n",
    "Analiza lexicala consta in parcurgerea cuvintelor din codul sursa si <b>tokenizarea</b> acestora. Prin tokenizare intelgem asocierea unui **tip** pentru fiecare cuvand din codul sursa. De exemplu:\n",
    "\n",
    "Cod sursa:\n",
    "> int a = 123;\n",
    "\n",
    "Tokeni:\n",
    "<ol>\n",
    "  <li>\"int\" = KEYWORD</li>\n",
    "  <li>\"a\" = IDENTIFIER</li>\n",
    "  <li>\"=\" = OPERATOR</li>\n",
    "  <li>\"123\" = LITERAL</li>\n",
    "  <li>\";\" = PUNCTUATOR</li>\n",
    "</ol>\n",
    "  \n",
    "Scopul analizei lexicale este de a sparge codul sursa in token si de a-i pasa analizatorului semantic.\n",
    "    \n",
    "### Analiza sintactica\n",
    "\n",
    "**DEX**: SINTÁXĂ, sintaxe, s. f. Parte a gramaticii care studiază funcțiile cuvintelor și ale propozițiilor în vorbire și care ***stabilește regulile de îmbinare*** a cuvintelor în propoziții și a propozițiilor în fraze.\n",
    "\n",
    "Analiza sintatica se ocupa cu parsarea tokenilor si verificarea sintactica a codului sursa (daca are sens inlantuirea tokenilor). Daca primim de la analizatorul lexical (KEYWORD IDENTIFIER OPERATOR LITERAL PUNCTUATOR => ar putea fi o instructiune). In schimb daca avem: LITERAL IDENTIFIER LITERAL PUNCTUATOR (de ex: 1233 radical 11;) nu are sens din punct de vedere sintactic, deci la acea linie exista o eroare.\n",
    "\n",
    "### Alte componente\n",
    "\n",
    "Exista si alte parti ale compilatoarelor pe care nu le vom discuta (de ex: analiza semantica -> observarea daca nu se poate ajunge niciodata undeva in cod (de ex: if (true) { ... } else { /\\* nu se va executa \\*/ ... }) si partea de optimizare (void f() { int i; for (i = 0; i < 100; i++) i = i + 1 - 1; **return 42;** }; for-ul nu are sens)\n",
    "\n",
    "## Notare\n",
    "\n",
    "La acest laborator veti avea de facut doua proiecte:\n",
    "\n",
    "<ul>\n",
    "    <li>Proiect analiza lexicala sau algoritmi</li>\n",
    "    <li>Proiect analiza sintactica sau algoritmi</li>\n",
    "</ul>\n",
    "\n",
    "Notarea:\n",
    "<ul>\n",
    "    <li>Daca nu prezentati nici un proiect, aveti nota 4</li>\n",
    "    <li>Daca prezentati un proiect si acesta este complet aveti nota 7</li>\n",
    "    <li>Daca prezentati doua proiecte si acestea sunt complete aveti nota 10</li>\n",
    "</ul>\n",
    "\n",
    "### Fiecare proiect are 3 puncte!\n",
    "\n",
    "### SURSELE VOR FI TESTATE ANTIPLAGIAT!\n",
    "\n",
    "Fiecare student va trebui sa incarce pe Github un folder (cu numele proiectului ales pe Google Sheets) avand urmatoarele componente:\n",
    "<ul>\n",
    "    <li>un fisier de tip <b>readme</b>, care va contine atat instructiuni de rulare ale programului, structura inputului, un exemplu de input si de output, cat si partile principale ale programului</li>\n",
    "    <li>un fisier de tip <b>makefile</b>, pentru a putea rula usor fisierul</li>\n",
    "    <li>un fisier de <b>intrare</b> si un fisier de <b>iesire</b></li>\n",
    "    <li>fisierele care contin <b>codul sursa</b></li>\n",
    "    <li>!! se va incarca doar codul sursa, nu si executabilul !!</li>\n",
    "</ul>\n",
    "\n",
    "### Fiecare proiect de algoritmica trebuie sa contina in output pasii intermediari <i>importanti</i>!\n",
    "\n",
    "Pentru fiecare proiect aveti mai multe variante de exercitii:\n",
    "<ol>\n",
    "    <li>trebuie sa alegeti un proiect din prima parte I (lexic) si un proiect din a doua parte II (sintactic)</li>\n",
    "    <li>proiecte care tin de analiza lexicala sau sintactica: (<b>I.1, I.2, I.3</b> sau <b>II.2, II.3, II.4</b>)</li>\n",
    "    <li>proiecte de algoritmica: (<b>I.4, I.5</b> sau <b>II.1, II.5, II.6</b>, II.7, II.8, II,9)</li>\n",
    "</ol>\n",
    "\n",
    "**Analiza lexicala sau algoritmi (maxim 5 persoane per proiect)**:\n",
    "<ol>\n",
    "    <li>Construirea unui lexer <b>manual</b> pentru un <i>limbaj de programare</i> folosind automate finite deterministe (DFA)</li>\n",
    "    <li>Construirea unui lexer <b>automat</b> pentru un <i>limbaj de programare</i> folosind lex sau o reimplementare a acestuia</li>\n",
    "    <li>Construirea unui lexer (<b>reimplementare lex</b>) folosind expresii regulate (<b>Regex</b>)</li>\n",
    "    <li>Translator stiva nedeterminist</li>\n",
    "    <li>Convertire expresie regulata in automat finit determinist (REGEX => DFA)</li>\n",
    "</ol>\n",
    "\n",
    "#### Analizator lexical (manual sau automat)\n",
    "\n",
    "Pentru prima tema trebuie sa scrieti un analizor lexical pentru un limbaj de programare la alegere. Scrieti analizorul sub forma unei functii care returneaza: \n",
    "\n",
    "<ul>\n",
    "    <li>tipul token-ului curent</li>\n",
    "    <li>lungimea sirului corespunzator din fisierul de intrare</li>\n",
    "    <li>linia din fisierul de intrare pe care se afla token-ul curent</li>\n",
    "    <li>pointer catre primul caracter al token-ulului curent</li>\n",
    "    <li>un mesaj de eroare atunci cand este intalnita o eroare lexicala.</li>\n",
    "</ul>    \n",
    "    \n",
    "Functia este apelata din programul principal, in care este citit un fisier de intrare care va fi scanat cu ajutorul acestei functii, astfel incat sa se afiseze toti token-ii care apar in fisierul de intrare. Atunci cand este apelata, functia de scanare:\n",
    "\n",
    "<ul>\n",
    "    <li>incepand de la pointerul curent (care initial indica catre primul caracter al fisierului de intrare) sare peste un nr de caractere egal cu lungimea token-ului anterior (initial aceasta lungime este 0);</li>\n",
    "    <li>sare peste spatii, tab-uri, linii noi, pana intalneste primul caracter diferit de acestea; seteaza pointerul curent astfel ca sa indice catre acest caracter;</li>\n",
    "    <li>identifica token-ul curent, ce corespunde sirului ce incepe cu caracterul depistat la pasul anterior; determina tipul acestuia si lungimea sirului corespunzator;</li>\n",
    "    <li>in cazul in care este intalnita o eroare lexicala, semneleaza aceasta printr-un mesaj, scaneaza fisierul de intrare in continuare, pana gaseste primul caracter de tip spatiu, linie noua, tab, seteaza pointerul curent catre acest caracter, seteza lungimea token-ului curent cu 0 (in felul acesta programul va afisa in continuare token-ii urmatori, fara sa se opreasca la prima eroare intalnita).</li>\n",
    "    <li>se opreste cu scanarea cand a intalnit sfarsitul fisierului de intrare.</li>\n",
    "</ul>\n",
    "\n",
    "Pentru cazul **automat** trebuie doar sa returnati pentru fiecare token structura de mai sus.\n",
    "\n",
    "#### Translator stiva nedeterminist\n",
    "Program care simuleaza functionarea unui translator stiva nedeterminist cu lambda-tranzitii. Programul citeste dintr-un fisier elementele unui translator stiva nedeterminist cu lambda-tranzitii oarecare (starile, starea initiala, starile finale, alfabetul de intrare, alfabetul de iesire, alfabetul stivei, simbolul initial al stivei, tranzitiile). Programul permite citirea unui nr. oarecare de siruri peste alfabetul de intrare al translatorului. Pentru fiecare astfel de sir se afiseaza toate iesirile (siruri peste alfabetul de iesire) corespunzatoare (Atentie! pot exista 0, 1 sau mai multe iesiri pt acelasi sir de intrare).\n",
    "\n",
    "#### REGEX => DFA (arbori)\n",
    "Primiti intr-un fisier text o expresie regulata care contine litere (a, b, c etc.), operatorii expresiilor regulate (| -> sau, \\* -> Kleene star) si paranteze si returneaza un DFA care recunoaste aceleasi cuvinte ca si expresia regulata primita. Exemplu de input: **(a|b)\\*abb**, output: automatul finit care recunoaste cuvintele care se termina cu **abb**. La aceasta problema trebuie sa afisati DFA-ul printr-un graf (text) cat si grafic (folosind eventual pachete existente ca **networkx** sau altele). Daca nu afisati si grafic, vi se va scadea 1 punct.\n",
    "\n",
    "**Analiza sintactica sau algoritmi (maxim 5 persoane per proiect)**:\n",
    "<ol>\n",
    "    <li>Algoritmul CYK</li>\n",
    "    <li>Recursive descent parser</li>\n",
    "    <li>DSL, folosind lex si yacc</li>\n",
    "    <li>Autoformatare pentru un limbaj de programare</li>\n",
    "    <li>Parser LL(1)</li>\n",
    "    <li>Parser LR(0)</li>\n",
    "    <li>Parser LL(k) <b>+1 pct</b></li>\n",
    "    <li>Parser LR(1) <b>+1 pct</b></li>\n",
    "    <li>Parser LALR(1) <b>+1 pct</b></li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "#### Algoritmul CYK\n",
    "Primiti intr-un fisier o gramatica independenta de context in forma normala Chomsky si o lista de cuvinte. Trebuie sa folositi algoritmul CYK pentru a spune daca fiecare cuvant poate fi generat de gramatica si in cazul in care se poate, trebuie sa afisati o derivare a acestuia (de ex: S => AA => ABA => ACCA => acca). La aceasta problema trebuie sa afisati atat derivarea cat si grafic (folosind eventual pachete existente ca **networkx** sau altele). Daca afisati si grafic, primiti 1 punct in plus. Daca afisati corect doar daca poate fi generat sau nu, fara derivare, vi se vor scadea 2 puncte.\n",
    "\n",
    "#### Recursive descent parser\n",
    "Primiti intr-un fisier text o gramatica si intr-un alt fisier o lista de cuvinte. Trebuie sa generati un cod care atunci cand este rulat, va parsa cel de-al doilea fisier si va indica daca este corect din punct de vedere sintactic. De exemplu scrieti un fisier in Python, care atunci cand primeste o gramatica, scrie intr-un fisier rdp.c cod de C cu tot cu main, il compileaza, si atunci cand este rulat, parseaza al doilea fisier.\n",
    "\n",
    "#### DSL, folosind lex si yacc\n",
    "La aceasta problema trebuie sa implementati un Domain Specific Language. Trebuie sa va definiti un limbaj de programare pe care sa il folositi pentru rezolvarea altor probleme. Daca alegeti aceasta problema trebuie sa discutam limbajul si sa va modific putin sintaxa. De exemplu sa presupunem ca vreti sa implementati limbajul C si pe langa acesta vreti o instructiune de tipul:\n",
    "\n",
    "> fore (x = a[0,n]) printf(\"%d\", x);\n",
    "\n",
    "Evident, nu exista fore in C, dar daca il intalniti, voi il puteti translata in:\n",
    "\n",
    "int i, x;\n",
    "\n",
    "for (i = 0; i < n; i++) { x = a[i]; printf(\"%d\", x); }\n",
    "\n",
    "#### Autoformatare pentru un limbaj de programare\n",
    "La aceasta problema intrarea este un fisier care contine cod (\\*.c sau \\*.java etc.) si voi trebuie sa il rescrieti frumos. De exemplu daca aveti un cod care initial arata asa:\n",
    "\n",
    "> #include<stdio.h> int main(int argc &emsp;,char** &nbsp;&nbsp;&nbsp;argv){int a=23; if(a>2) {a++;} &nbsp;&nbsp;return a;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}\n",
    "\n",
    "Dupa ce apelati programul vostru cu acel input, trebuie sa afisati:\n",
    "\n",
    "#include<stdio.h> \n",
    "\n",
    "int main(int argc, char** argv) { \n",
    "\n",
    "&emsp;int a = 23;\n",
    "\n",
    "&emsp;if(a > 2) {\n",
    "\n",
    "&emsp;&emsp; a++;\n",
    "\n",
    "&emsp;}\n",
    "\n",
    "&emsp;return a;\n",
    "\n",
    "}\n",
    "\n",
    "#### Parser (LL(1), LR(0), LL(k), LR(1), LALR(1))\n",
    "La aceste probleme primiti o gramatica independenta de context (nu neaparat in forma normala Chomsky) si un cuvant si trebuie sa spuneti daca acel cuvant este recunoscut de gramatica folosind acel parser. De asemenea, trebuie sa afisati si pasii intermediari ai algoritmului pentru a putea verifica corectitudinea partiala a programului. La aceste probleme trebuie sa aratati si cum functioneaza parsarea unui cuvant (algoritmul care foloseste stiva).\n",
    "\n",
    "### Proiecte alternative:\n",
    "\n",
    "#### Administrator website + 1 singur proiect\n",
    "\n",
    "Un singur student per grupa poate alege sa se ocupe de un **website** care va descarca proiectul de pe Github si va face o interfata pentru a putea vedea si verifica mai usor proiectele. Daca exista mai multi studenti care vor sa administreze site-ul, acestia vor face o versiune preliminara a acestuia, si va fi ales cel care are interfata mai clara. Daca acel student va alege sa se ocupe de website, acesta va trebui obligatoriu sa mai aleaga un singur proiect din cele de la I si II (nu puteti primi nota fara sa faceti un proiect legat de TC).\n",
    "\n",
    "Administratorul site-ului va lua legatura cu studentii grupei si le va specifica modul in care trebuie sa salveze fisiere (daca au o singura intrare, daca au mai multe intrari, daca au poze etc.) in directorul lor de pe Github. Acesta va face si interfata grafica in care va specifica: un *readme* care spune cum trebuie dat input-ul - care se va lua de pe Github, un exemplu de input scris deja, eventual un form, care la submit afiseaza pe aceiasi pagina si output-ul (fie doar text, fie si cu imagini, depinzand de problema aleasa), va pune timeout pe executabil (in cazul buclelor infinite etc.).\n",
    "\n",
    "#### Cercetare\n",
    "\n",
    "Implementare parser pentru o expresie regulata bidimensionala si afisarea acesteia pe ecran. Daca sunt persoane interesate vom discuta mai multe la laborator despre aceasta problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalare sly\n",
    "\n",
    "Pentru a instala python-sly (care este o reimplementare a programelor lex si yacc) trebuie sa aveti instalat python3 si apoi trebuie sa instalati pachetul sly: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```$ sudo apt-get update && sudo apt-get install python3.7 python3-pip && sudo pip install sly```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificare instalare\n",
    "\n",
    "Pentru a verifica instalarea rulati urmatorul cod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sly import Lexer, Parser\n",
    "\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructia unui lexer simplu\n",
    "\n",
    "Pentru a construi un lexer simplu trebuie sa definim o clasa si sa derivam din clasa Lexer. Dupa aceasta trebuie sa definim tipurile de tokeni folositi si sa definim o expresie regulata pentru fiecare dintre acestia. Apoi trebuie sa rulam lexer-ul pentru a primi fiecare token dintr-un input.\n",
    "\n",
    "Ca o prima problema, sa spunem ca vrem sa citim fie numere fie cuvinte si atunci cand am ajuns la un spatiu sa spunem daca ceea ce am citit este un cuvant sau un numar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordOrNumberLexer(Lexer):\n",
    "    tokens = { WORD, NUMBER }   # avem doar doua tipuri de tokeni\n",
    "    ignore = ' \\t'              # nu ne intereseaza spatiile sau tab-ul\n",
    "\n",
    "    # Expresii regulate pentru fiecare tip de token\n",
    "    WORD = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    NUMBER = r'\\d+'\n",
    "\n",
    "    # Puteti sa ignorati si pattern-uri, nu doar litere simple cum avem mai sus\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    # Puteti sa numarati cate newline-uri aveti in fisierul text\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    # Daca apare altceva inafara de expresiile regulate definite mai sus\n",
    "    # afisam o eroare intrucat nu putem parsa mai departe.\n",
    "    # Programul de mai jos doar trece peste aceasta eroare.\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avand clasa definita, urmeaza sa implementam functia main care citeste de la tastatura input-uri si afiseaza pentru fiecare dintre acestea in ce categorie e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc > 1231\n",
      "Token(type='NUMBER', value='1231', lineno=1, index=0)\n",
      "calc > 123 kjkj 1211 1  jk jlj   kj lj lkj kllll\n",
      "Token(type='NUMBER', value='123', lineno=1, index=0)\n",
      "Token(type='WORD', value='kjkj', lineno=1, index=4)\n",
      "Token(type='NUMBER', value='1211', lineno=1, index=9)\n",
      "Token(type='NUMBER', value='1', lineno=1, index=14)\n",
      "Token(type='WORD', value='jk', lineno=1, index=17)\n",
      "Token(type='WORD', value='jlj', lineno=1, index=20)\n",
      "Token(type='WORD', value='kj', lineno=1, index=26)\n",
      "Token(type='WORD', value='lj', lineno=1, index=29)\n",
      "Token(type='WORD', value='lkj', lineno=1, index=32)\n",
      "Token(type='WORD', value='kllll', lineno=1, index=36)\n"
     ]
    }
   ],
   "source": [
    "lexer = WordOrNumberLexer()\n",
    "while True:\n",
    "    try:\n",
    "        text = input('calc > ')\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    if text:\n",
    "        for token in lexer.tokenize(text):\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa modificam clasa pentru a numara cate cuvinte si cate numere avem.\n",
    "De asemenea nu mai citim de la tastatura ci vom presupune ca am citit din fisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordOrNumberLexer(Lexer):\n",
    "    tokens = { WORD, NUMBER }   # avem doar doua tipuri de tokeni\n",
    "    ignore = ' \\t'              # nu ne intereseaza spatiile sau tab-ul\n",
    "\n",
    "    def __init__(self):\n",
    "        self.words_count = 0\n",
    "        self.numbers_count = 0\n",
    "        \n",
    "        self.words = []\n",
    "        self.numbers = []\n",
    "    \n",
    "    # Expresii regulate pentru fiecare tip de token\n",
    "    WORD = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    NUMBER = r'\\d+'\n",
    "    \n",
    "    def WORD(self, w):\n",
    "        self.words_count += 1\n",
    "        self.words.append(w.value)\n",
    "        \n",
    "        return w\n",
    "        \n",
    "    def NUMBER(self, n):\n",
    "        self.numbers_count += 1\n",
    "        self.numbers.append(n.value)\n",
    "        \n",
    "        return n\n",
    "    \n",
    "    # Puteti sa ignorati si pattern-uri, nu doar litere simple cum avem mai sus\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    # Puteti sa numarati cate newline-uri aveti in fisierul text\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    # Daca apare altceva inafara de expresiile regulate definite mai sus\n",
    "    # afisam o eroare intrucat nu putem parsa mai departe.\n",
    "    # Programul de mai jos doar trece pe aceasta eroare.\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='NUMBER', value='1337', lineno=1, index=0)\n",
      "Token(type='WORD', value='test', lineno=1, index=5)\n",
      "Token(type='WORD', value='other', lineno=1, index=10)\n",
      "Token(type='NUMBER', value='1122', lineno=1, index=16)\n",
      "Token(type='NUMBER', value='1', lineno=1, index=21)\n",
      "Token(type='NUMBER', value='3', lineno=1, index=23)\n",
      "Token(type='WORD', value='here', lineno=1, index=25)\n",
      "\n",
      "Numbers_count = 4, Numbers = ['1337', '1122', '1', '3']\n",
      "Words_count = 3, Words = ['test', 'other', 'here']\n"
     ]
    }
   ],
   "source": [
    "input_text = '1337 test other 1122 1 3 here'\n",
    "lexer = WordOrNumberLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "\n",
    "print()    \n",
    "print('Numbers_count = {}, Numbers = {}'.format(lexer.numbers_count, lexer.numbers))\n",
    "print('Words_count = {}, Words = {}'.format(lexer.words_count, lexer.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dupa cum puteti vedea numerele salvate in lista numbers sunt inca string-uri. Daca vreti sa obtineti suma lor trebuie mai intai sa le convertiti la numere intregi (folosind functia int('123')) si apoi de fiecare data cand cititi un numar sa il adaugati la suma. Alternativ, puteti face asta si in programul principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Primind un for-uri si if-uri afisati tokenii lor. Pentru a usura implementarea la acest moment, vom considera ca avem doua cuvinte cheie: for si if; instructiunile sunt date ca a=b si conditiile a<b; si aveti paranteze si acolade deschise si inchise. De exemplu daca primiti for(i=0; i<9; i=i+1) { if(i>2) a=a+i; if (i<3) a=a+2; } trebuie sa afisati FOR PDESCHISA INSTRUCTIUNE CONDITIE INSTRUCTIUNE PINCHISA ADESCHISA IF PDESCHISA CONDITIE PINCHISA ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForIfLexer(Lexer):\n",
    "    tokens = { FOR_TOKEN, IF_TOKEN, INSTRUCTION, CONDITION, OPENP, CLOSEDP, OPENCB, CLOSEDCB }\n",
    "    ignore = ' \\t'\n",
    "    \n",
    "    # Expresii regulate pentru fiecare tip de token\n",
    "    FOR_TOKEN = r'for'\n",
    "    IF_TOKEN = r'if'\n",
    "    INSTRUCTION = r'[a-zA-Z_][a-zA-Z0-9_]*\\=([a-zA-Z_][a-zA-Z0-9_]*|\\d+)(\\+([a-zA-Z_][a-zA-Z0-9_]*|\\d+))?;?'\n",
    "    CONDITION = r'[a-zA-Z_][a-zA-Z0-9_]*(<|>|<\\=|>\\=)([a-zA-Z_][a-zA-Z0-9_]*|\\d+);?'\n",
    "    OPENP = r'\\('\n",
    "    CLOSEDP = r'\\)'\n",
    "    OPENCB = r'{'\n",
    "    CLOSEDCB = r'}'\n",
    "    \n",
    "    # Puteti sa ignorati si pattern-uri, nu doar litere simple cum avem mai sus\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    # Puteti sa numarati cate newline-uri aveti in fisierul text\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    # Daca apare altceva inafara de expresiile regulate definite mai sus\n",
    "    # afisam o eroare intrucat nu putem parsa mai departe.\n",
    "    # Programul de mai jos doar trece pe aceasta eroare.\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='FOR_TOKEN', value='for', lineno=1, index=0)\n",
      "Token(type='OPENP', value='(', lineno=1, index=3)\n",
      "Token(type='INSTRUCTION', value='i=0;', lineno=1, index=4)\n",
      "Token(type='CONDITION', value='i<9;', lineno=1, index=9)\n",
      "Token(type='INSTRUCTION', value='i=i+1', lineno=1, index=14)\n",
      "Token(type='CLOSEDP', value=')', lineno=1, index=19)\n",
      "Token(type='OPENCB', value='{', lineno=1, index=21)\n",
      "Token(type='IF_TOKEN', value='if', lineno=1, index=23)\n",
      "Token(type='OPENP', value='(', lineno=1, index=25)\n",
      "Token(type='CONDITION', value='i>2', lineno=1, index=26)\n",
      "Token(type='CLOSEDP', value=')', lineno=1, index=29)\n",
      "Token(type='INSTRUCTION', value='a=a+i;', lineno=1, index=31)\n",
      "Token(type='IF_TOKEN', value='if', lineno=1, index=38)\n",
      "Token(type='OPENP', value='(', lineno=1, index=41)\n",
      "Token(type='CONDITION', value='i<3', lineno=1, index=42)\n",
      "Token(type='CLOSEDP', value=')', lineno=1, index=45)\n",
      "Token(type='INSTRUCTION', value='a=a+2;', lineno=1, index=47)\n",
      "Token(type='CLOSEDCB', value='}', lineno=1, index=54)\n",
      "\n",
      "FOR_TOKEN OPENP INSTRUCTION CONDITION INSTRUCTION CLOSEDP OPENCB IF_TOKEN OPENP CONDITION CLOSEDP INSTRUCTION IF_TOKEN OPENP CONDITION CLOSEDP INSTRUCTION CLOSEDCB\n"
     ]
    }
   ],
   "source": [
    "input_text = 'for(i=0; i<9; i=i+1) { if(i>2) a=a+i; if (i<3) a=a+2; } '\n",
    "lexer = ForIfLexer()\n",
    "\n",
    "tokens_value = []\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "    tokens_value.append(token.type)\n",
    "\n",
    "print()\n",
    "print(' '.join(tokens_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Construiti un lexer care primeste numere in baza b (2 <= b <= 10) si **converteste fiecare numar in baza 10**.\n",
    "\n",
    "Numerele primite sunt de forma baza\\\\$numar. Daca baza = 10, 10\\\\$ poate lipsi si poate ramana doar numarul.\n",
    "\n",
    "Exemple de numere: 2\\\\$1101 4\\\\$1132 006\\\\$5155 10\\\\$1918. Ultimul numar poate fi dat si ca 1918.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversionLexer(Lexer):\n",
    "    tokens = {}\n",
    "    ignore = ' \\t'\n",
    "    \n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConversionLexer' has no attribute '_master_re'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-d19249ab4899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversionLexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/lex.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, lineno, index)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0m_set_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/lex.py\u001b[0m in \u001b[0;36m_set_state\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mnonlocal\u001b[0m \u001b[0m_ignored_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_master_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_token_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_literals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_remapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0m_ignored_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignored_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0m_master_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0m_token_funcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ConversionLexer' has no attribute '_master_re'"
     ]
    }
   ],
   "source": [
    "input_text = '2$1101 4$1132 006$5155 10$1918'\n",
    "lexer = ConversionLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Notatia condensata poate fi folosita pentru a utiliza mai putina memorie atunci cand dorim sa salvam numere. Principiul condensarii se bazeaza pe faptul ca putem reprezenta un numar relativ la o baza. De exemplu daca avem urmatoarea secventa de numere si baza = 150:\n",
    "\n",
    "> S = 149 154 154 154 147 147 145 152 153 150 150 148 162\n",
    "\n",
    "Putem condensa numerele in functie de baza:\n",
    "\n",
    "> S = 150-1 150+4 150+4 150+4 150-3 150-3 150-5 150+2 150+3 150 150 150-2 150+12\n",
    "\n",
    "Atunci cand sunt mai multe aparitii ale aceluiasi numar punem + (respectiv -) de cate ori apare numarul respectiv.\n",
    "Daca numarul este chiar baza punem 0.\n",
    "\n",
    "> CS = 150\\\\$-1b+++4b--3b-5b+2b+3b00b-2b+12b\n",
    "\n",
    "Unde b reprezinta un spatiu.\n",
    "\n",
    "Voi trebuie sa scrieti un program care va converteste o expresie in din forma CS in S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondensedNotationLexer(Lexer):\n",
    "    tokens = {}\n",
    "    ignore = ' \\t'\n",
    "    \n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConversionLexer' has no attribute '_master_re'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-930c73148f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversionLexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/lex.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, lineno, index)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0m_set_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/lex.py\u001b[0m in \u001b[0;36m_set_state\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mnonlocal\u001b[0m \u001b[0m_ignored_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_master_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_token_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_literals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_remapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0m_ignored_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignored_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0m_master_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0m_token_funcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ConversionLexer' has no attribute '_master_re'"
     ]
    }
   ],
   "source": [
    "input_text = '150$-1 +++4 --3 -5 +2 +3 00 -2 +12 '\n",
    "lexer = CondensedNotationLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
