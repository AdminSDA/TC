{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laborator I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalare sly\n",
    "\n",
    "Pentru a instala python-sly (care este o reimplementare a programelor lex si yacc) trebuie sa aveti instalat python3 si apoi trebuie sa instalati pachetul sly: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```$ sudo apt-get update && sudo apt-get install python3.7 python3-pip && sudo pip install sly```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificare instalare\n",
    "\n",
    "Pentru a verifica instalarea rulati urmatorul cod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sly import Lexer, Parser\n",
    "\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructia unui lexer simplu\n",
    "\n",
    "Pentru a construi un lexer simplu trebuie sa definim o clasa si sa derivam din clasa Lexer. Dupa aceasta trebuie sa definim tipurile de tokeni folositi si sa definim o expresie regulata (testare regex: https://pythex.org/) pentru fiecare dintre acestia. Apoi trebuie sa rulam lexer-ul pentru a primi fiecare token dintr-un input.\n",
    "\n",
    "Ca o prima problema, sa spunem ca vrem sa citim fie numere fie cuvinte si atunci cand am ajuns la un spatiu sa spunem daca ceea ce am citit este un cuvant sau un numar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordOrNumberLexer(Lexer):\n",
    "    tokens = { WORD, NUMBER }   # avem doar doua tipuri de tokeni\n",
    "    ignore = ' \\t'              # nu ne intereseaza spatiile sau tab-ul\n",
    "\n",
    "    # Expresii regulate pentru fiecare tip de token\n",
    "    WORD = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    NUMBER = r'\\d+'\n",
    "\n",
    "    # Puteti sa ignorati si pattern-uri, nu doar litere simple cum avem mai sus\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    # Puteti sa numarati cate newline-uri aveti in fisierul text\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    # Daca apare altceva inafara de expresiile regulate definite mai sus\n",
    "    # afisam o eroare intrucat nu putem parsa mai departe.\n",
    "    # Programul de mai jos doar trece peste aceasta eroare.\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avand clasa definita, urmeaza sa implementam functia main care citeste de la tastatura input-uri si afiseaza pentru fiecare dintre acestea in ce categorie e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input > test\n",
      "Token(type='WORD', value='test', lineno=1, index=0)\n",
      "\n",
      "input > 123\n",
      "Token(type='NUMBER', value='123', lineno=1, index=0)\n",
      "\n",
      "input > test 123 ana are 4 mere\n",
      "Token(type='WORD', value='test', lineno=1, index=0)\n",
      "Token(type='NUMBER', value='123', lineno=1, index=5)\n",
      "Token(type='WORD', value='ana', lineno=1, index=9)\n",
      "Token(type='WORD', value='are', lineno=1, index=13)\n",
      "Token(type='NUMBER', value='4', lineno=1, index=17)\n",
      "Token(type='WORD', value='mere', lineno=1, index=19)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lexer = WordOrNumberLexer()\n",
    "while True:\n",
    "    try:\n",
    "        text = input('input > ')\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    if text:\n",
    "        for token in lexer.tokenize(text):\n",
    "            print(token)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sa modificam clasa pentru a numara cate cuvinte si cate numere avem.\n",
    "De asemenea nu mai citim de la tastatura ci vom presupune ca am citit din fisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordOrNumberLexer(Lexer):\n",
    "    tokens = { WORD, NUMBER }   # avem doar doua tipuri de tokeni\n",
    "    ignore = ' \\t'              # nu ne intereseaza spatiile sau tab-ul\n",
    "\n",
    "    def __init__(self):\n",
    "        self.words_count = 0\n",
    "        self.numbers_count = 0\n",
    "        \n",
    "        self.words = []\n",
    "        self.numbers = []\n",
    "    \n",
    "    # Expresii regulate pentru fiecare tip de token\n",
    "    WORD = r'[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    NUMBER = r'\\d+'\n",
    "    \n",
    "    def WORD(self, w):\n",
    "        self.words_count += 1\n",
    "        self.words.append(w.value)\n",
    "        \n",
    "        return w\n",
    "        \n",
    "    def NUMBER(self, n):\n",
    "        self.numbers_count += 1\n",
    "        self.numbers.append(n.value)\n",
    "        \n",
    "        return n\n",
    "    \n",
    "    # Puteti sa ignorati si pattern-uri, nu doar litere simple cum avem mai sus\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    # Puteti sa numarati cate newline-uri aveti in fisierul text\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    # Daca apare altceva inafara de expresiile regulate definite mai sus\n",
    "    # afisam o eroare intrucat nu putem parsa mai departe.\n",
    "    # Programul de mai jos doar trece pe aceasta eroare.\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='NUMBER', value='1337', lineno=1, index=0)\n",
      "Token(type='WORD', value='test', lineno=1, index=5)\n",
      "Token(type='WORD', value='other', lineno=1, index=10)\n",
      "Token(type='NUMBER', value='1122', lineno=1, index=16)\n",
      "Token(type='NUMBER', value='1', lineno=1, index=21)\n",
      "Token(type='NUMBER', value='3', lineno=1, index=23)\n",
      "Token(type='WORD', value='here', lineno=1, index=25)\n",
      "\n",
      "Numbers_count =  4 Numbers =  ['1337', '1122', '1', '3']\n",
      "Words_count =  3 Words =  ['test', 'other', 'here']\n"
     ]
    }
   ],
   "source": [
    "input_text = '1337 test other 1122 1 3 here'\n",
    "lexer = WordOrNumberLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "\n",
    "print()    \n",
    "print('Numbers_count = ', lexer.numbers_count, 'Numbers = ', lexer.numbers)\n",
    "print('Words_count = ', lexer.words_count, 'Words = ', lexer.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dupa cum puteti vedea numerele salvate in lista numbers sunt inca string-uri. Daca vreti sa obtineti suma lor trebuie mai intai sa le convertiti la numere intregi (folosind functia int('123')) si apoi de fiecare data cand cititi un numar sa il adaugati la suma. Alternativ, puteti face asta si in programul principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Primind un for-uri si if-uri afisati tokenii lor. Pentru a usura implementarea la acest moment, vom considera ca avem doua cuvinte cheie: for si if; instructiunile sunt date ca a=b si conditiile a<b; si aveti paranteze si acolade deschise si inchise. De exemplu daca primiti for(i=0; i<9; i=i+1) { if(i>2) a=a+i; if (i<3) a=a+2; } trebuie sa afisati FOR PDESCHISA INSTRUCTIUNE CONDITIE INSTRUCTIUNE PINCHISA ADESCHISA IF PDESCHISA CONDITIE PINCHISA ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForIfLexer(Lexer):\n",
    "    tokens = { FOR_TOKEN, IF_TOKEN, INSTRUCTION, CONDITION, OPENP, CLOSEDP, OPENCB, CLOSEDCB }\n",
    "    ignore = ' \\t'\n",
    "    \n",
    "    # Expresii regulate pentru fiecare tip de token\n",
    "    FOR_TOKEN = r'for'\n",
    "    IF_TOKEN = r'if'\n",
    "    INSTRUCTION = r'[a-zA-Z_][a-zA-Z0-9_]*\\=([a-zA-Z_][a-zA-Z0-9_]*|\\d+)(\\+([a-zA-Z_][a-zA-Z0-9_]*|\\d+))?;?'\n",
    "    CONDITION = r'[a-zA-Z_][a-zA-Z0-9_]*(<|>|<\\=|>\\=)([a-zA-Z_][a-zA-Z0-9_]*|\\d+);?'\n",
    "    OPENP = r'\\('\n",
    "    CLOSEDP = r'\\)'\n",
    "    OPENCB = r'{'\n",
    "    CLOSEDCB = r'}'\n",
    "    \n",
    "    # Puteti sa ignorati si pattern-uri, nu doar litere simple cum avem mai sus\n",
    "    ignore_newline = r'\\n+'\n",
    "\n",
    "    # Puteti sa numarati cate newline-uri aveti in fisierul text\n",
    "    def ignore_newline(self, t):\n",
    "        self.lineno += t.value.count('\\n')\n",
    "\n",
    "    # Daca apare altceva inafara de expresiile regulate definite mai sus\n",
    "    # afisam o eroare intrucat nu putem parsa mai departe.\n",
    "    # Programul de mai jos doar trece pe aceasta eroare.\n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(type='FOR_TOKEN', value='for', lineno=1, index=0)\n",
      "Token(type='OPENP', value='(', lineno=1, index=3)\n",
      "Token(type='INSTRUCTION', value='i=0;', lineno=1, index=4)\n",
      "Token(type='CONDITION', value='i<9;', lineno=1, index=9)\n",
      "Token(type='INSTRUCTION', value='i=i+1', lineno=1, index=14)\n",
      "Token(type='CLOSEDP', value=')', lineno=1, index=19)\n",
      "Token(type='OPENCB', value='{', lineno=1, index=21)\n",
      "Token(type='IF_TOKEN', value='if', lineno=1, index=23)\n",
      "Token(type='OPENP', value='(', lineno=1, index=25)\n",
      "Token(type='CONDITION', value='i>2', lineno=1, index=26)\n",
      "Token(type='CLOSEDP', value=')', lineno=1, index=29)\n",
      "Token(type='INSTRUCTION', value='a=a+i;', lineno=1, index=31)\n",
      "Token(type='IF_TOKEN', value='if', lineno=1, index=38)\n",
      "Token(type='OPENP', value='(', lineno=1, index=41)\n",
      "Token(type='CONDITION', value='i<3', lineno=1, index=42)\n",
      "Token(type='CLOSEDP', value=')', lineno=1, index=45)\n",
      "Token(type='INSTRUCTION', value='a=a+2;', lineno=1, index=47)\n",
      "Token(type='CLOSEDCB', value='}', lineno=1, index=54)\n",
      "\n",
      "FOR_TOKEN OPENP INSTRUCTION CONDITION INSTRUCTION CLOSEDP OPENCB IF_TOKEN OPENP CONDITION CLOSEDP INSTRUCTION IF_TOKEN OPENP CONDITION CLOSEDP INSTRUCTION CLOSEDCB\n"
     ]
    }
   ],
   "source": [
    "input_text = 'for(i=0; i<9; i=i+1) { if(i>2) a=a+i; if (i<3) a=a+2; } '\n",
    "lexer = ForIfLexer()\n",
    "\n",
    "tokens_value = []\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)\n",
    "    tokens_value.append(token.type)\n",
    "\n",
    "print()\n",
    "print(' '.join(tokens_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Construiti un lexer care primeste numere in baza b (2 <= b <= 10) si **converteste fiecare numar in baza 10**.\n",
    "\n",
    "Numerele primite sunt de forma baza\\\\$numar. Daca baza = 10, 10\\\\$ poate lipsi si poate ramana doar numarul.\n",
    "\n",
    "Exemple de numere: 2\\\\$1101 4\\\\$1132 006\\\\$5155 10\\\\$1918. Ultimul numar poate fi dat si ca 1918.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversionLexer(Lexer):\n",
    "    tokens = {}\n",
    "    ignore = ' \\t'\n",
    "    \n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ConversionLexer' has no attribute '_master_re'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d19249ab4899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversionLexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/lex.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, lineno, index)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0m_set_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sly/lex.py\u001b[0m in \u001b[0;36m_set_state\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mnonlocal\u001b[0m \u001b[0m_ignored_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_master_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_token_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_literals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_remapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0m_ignored_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignored_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0m_master_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_re\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0m_ignore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0m_token_funcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ConversionLexer' has no attribute '_master_re'"
     ]
    }
   ],
   "source": [
    "input_text = '2$1101 4$1132 006$5155 10$1918'\n",
    "lexer = ConversionLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercitiu\n",
    "\n",
    "Notatia condensata poate fi folosita pentru a utiliza mai putina memorie atunci cand dorim sa salvam numere. Principiul condensarii se bazeaza pe faptul ca putem reprezenta un numar relativ la o baza. De exemplu daca avem urmatoarea secventa de numere si baza = 150:\n",
    "\n",
    "> S = 149 154 154 154 147 147 145 152 153 150 150 148 162\n",
    "\n",
    "Putem condensa numerele in functie de baza:\n",
    "\n",
    "> S = 150-1 150+4 150+4 150+4 150-3 150-3 150-5 150+2 150+3 150 150 150-2 150+12\n",
    "\n",
    "Atunci cand sunt mai multe aparitii ale aceluiasi numar punem + (respectiv -) de cate ori apare numarul respectiv.\n",
    "Daca numarul este chiar baza punem 0.\n",
    "\n",
    "> CS = 150\\\\$-1b+++4b--3b-5b+2b+3b00b-2b+12b\n",
    "\n",
    "Unde b reprezinta un spatiu.\n",
    "\n",
    "Voi trebuie sa scrieti un program care va converteste o expresie in din forma CS in S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondensedNotationLexer(Lexer):\n",
    "    tokens = {}\n",
    "    ignore = ' \\t'\n",
    "    \n",
    "    def error(self, t):\n",
    "        print(\"Illegal character '%s'\" % t.value[0])\n",
    "        self.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '150$-1 +++4 --3 -5 +2 +3 00 -2 +12 '\n",
    "lexer = CondensedNotationLexer()\n",
    "\n",
    "for token in lexer.tokenize(input_text):\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
